"""
NLP Service –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embeddings
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç spaCy –¥–ª—è –Ω–µ–º–µ—Ü–∫–æ–≥–æ —è–∑—ã–∫–∞ –∏ sentence-transformers –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
"""

import os
from typing import List, Dict, Any, Optional, Tuple
import spacy
from sentence_transformers import SentenceTransformer
import numpy as np


class NLPService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è NLP –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤"""

    def __init__(self):
        """Initialize NLP Service"""
        self.spacy_model_name = os.getenv("SPACY_MODEL", "de_core_news_lg")
        self.transformer_model_name = os.getenv(
            "SENTENCE_TRANSFORMER_MODEL",
            "paraphrase-multilingual-MiniLM-L12-v2"
        )

        self.nlp = None
        self.embedder = None
        self.initialized = False

    async def initialize(self):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLP –º–æ–¥–µ–ª–µ–π

        –ó–∞–≥—Ä—É–∂–∞–µ—Ç:
        - spaCy –º–æ–¥–µ–ª—å –¥–ª—è –Ω–µ–º–µ—Ü–∫–æ–≥–æ —è–∑—ã–∫–∞ (—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, POS, NER)
        - Sentence-Transformer –º–æ–¥–µ–ª—å –¥–ª—è embeddings
        """
        try:
            # –ó–∞–≥—Ä—É–∑–∏—Ç—å spaCy –º–æ–¥–µ–ª—å
            print(f"üîÑ Loading spaCy model: {self.spacy_model_name}")
            try:
                self.nlp = spacy.load(self.spacy_model_name)
                print(f"‚úÖ spaCy model loaded: {self.spacy_model_name}")
            except OSError:
                print(f"‚ö†Ô∏è  spaCy model {self.spacy_model_name} not found")
                print(f"üí° Install with: python -m spacy download {self.spacy_model_name}")
                # Fallback –∫ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
                try:
                    self.nlp = spacy.load("de_core_news_sm")
                    print("‚úÖ Fallback to de_core_news_sm")
                except:
                    print("‚ö†Ô∏è  No German spaCy model available")

            # –ó–∞–≥—Ä—É–∑–∏—Ç—å Sentence-Transformer –º–æ–¥–µ–ª—å
            print(f"üîÑ Loading Sentence-Transformer: {self.transformer_model_name}")
            self.embedder = SentenceTransformer(self.transformer_model_name)
            print(f"‚úÖ Sentence-Transformer loaded")

            self.initialized = True
            print("‚úÖ NLP Service initialized")

        except Exception as e:
            print(f"‚ö†Ô∏è  NLP Service initialization failed: {e}")
            self.initialized = False

    def is_ready(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–∏—Å–∞"""
        return self.initialized and self.nlp is not None and self.embedder is not None

    # === Tokenization & Preprocessing ===

    def tokenize(self, text: str) -> List[str]:
        """
        –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞

        :param text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        :return: –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤
        """
        if not self.is_ready():
            # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç—É—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é
            return text.split()

        doc = self.nlp(text)
        return [token.text for token in doc]

    def lemmatize(self, text: str) -> List[str]:
        """
        –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞

        :param text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        :return: –°–ø–∏—Å–æ–∫ –ª–µ–º–º
        """
        if not self.is_ready():
            return text.split()

        doc = self.nlp(text)
        return [token.lemma_ for token in doc]

    def remove_stopwords(self, text: str) -> str:
        """
        –£–¥–∞–ª–∏—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞

        :param text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        :return: –¢–µ–∫—Å—Ç –±–µ–∑ —Å—Ç–æ–ø-—Å–ª–æ–≤
        """
        if not self.is_ready():
            return text

        doc = self.nlp(text)
        filtered = [token.text for token in doc if not token.is_stop]
        return " ".join(filtered)

    # === Named Entity Recognition ===

    def extract_entities(self, text: str) -> List[Dict[str, Any]]:
        """
        –ò–∑–≤–ª–µ—á—å –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏

        :param text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        :return: –°–ø–∏—Å–æ–∫ —Å—É—â–Ω–æ—Å—Ç–µ–π —Å —Ç–∏–ø–∞–º–∏
        """
        if not self.is_ready():
            return []

        doc = self.nlp(text)
        entities = []

        for ent in doc.ents:
            entities.append({
                "text": ent.text,
                "label": ent.label_,
                "start": ent.start_char,
                "end": ent.end_char
            })

        return entities

    def extract_legal_references(self, text: str) -> List[str]:
        """
        –ò–∑–≤–ª–µ—á—å —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Å—Å—ã–ª–∫–∏ (¬ß, Art., etc.)

        :param text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        :return: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
        """
        import re

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å—Å—ã–ª–æ–∫
        patterns = [
            r'¬ß\s*\d+[a-z]?',  # ¬ß 5, ¬ß 29a
            r'Art\.\s*\d+',     # Art. 12
            r'Abs\.\s*\d+',     # Abs. 1
            r'Satz\s*\d+',      # Satz 2
        ]

        references = []
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            references.extend(matches)

        return list(set(references))  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ

    # === Embeddings ===

    def generate_embedding(self, text: str) -> Optional[List[float]]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å embedding –¥–ª—è —Ç–µ–∫—Å—Ç–∞

        :param text: –¢–µ–∫—Å—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        :return: –í–µ–∫—Ç–æ—Ä embeddings –∏–ª–∏ None
        """
        if not self.is_ready():
            return None

        try:
            embedding = self.embedder.encode(text, convert_to_numpy=True)
            return embedding.tolist()
        except Exception as e:
            print(f"‚ö†Ô∏è  Embedding generation failed: {e}")
            return None

    def generate_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å embeddings –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤

        –ë–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —á–µ–º –ø–æ –æ–¥–Ω–æ–º—É
        :param texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
        :return: –°–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤
        """
        if not self.is_ready():
            return []

        try:
            embeddings = self.embedder.encode(texts, convert_to_numpy=True)
            return embeddings.tolist()
        except Exception as e:
            print(f"‚ö†Ô∏è  Batch embedding failed: {e}")
            return []

    # === Similarity ===

    def calculate_similarity(
        self,
        text1: str,
        text2: str
    ) -> float:
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É —Ç–µ–∫—Å—Ç–∞–º–∏

        :param text1: –ü–µ—Ä–≤—ã–π —Ç–µ–∫—Å—Ç
        :param text2: –í—Ç–æ—Ä–æ–π —Ç–µ–∫—Å—Ç
        :return: –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (0-1)
        """
        emb1 = self.generate_embedding(text1)
        emb2 = self.generate_embedding(text2)

        if emb1 is None or emb2 is None:
            return 0.0

        # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        return float(np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2)))

    def find_most_similar(
        self,
        query: str,
        candidates: List[str],
        top_k: int = 5
    ) -> List[Tuple[int, float]]:
        """
        –ù–∞–π—Ç–∏ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–µ —Ç–µ–∫—Å—Ç—ã

        :param query: –ó–∞–ø—Ä–æ—Å
        :param candidates: –°–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        :param top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        :return: –°–ø–∏—Å–æ–∫ (–∏–Ω–¥–µ–∫—Å, score)
        """
        if not self.is_ready() or not candidates:
            return []

        query_emb = self.generate_embedding(query)
        if query_emb is None:
            return []

        candidate_embs = self.generate_embeddings_batch(candidates)
        if not candidate_embs:
            return []

        # –í—ã—á–∏—Å–ª–∏—Ç—å –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–ª—è –≤—Å–µ—Ö
        similarities = []
        query_norm = np.linalg.norm(query_emb)

        for idx, cand_emb in enumerate(candidate_embs):
            cand_norm = np.linalg.norm(cand_emb)
            if query_norm == 0 or cand_norm == 0:
                similarity = 0.0
            else:
                similarity = float(
                    np.dot(query_emb, cand_emb) / (query_norm * cand_norm)
                )
            similarities.append((idx, similarity))

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
        similarities.sort(key=lambda x: x[1], reverse=True)

        return similarities[:top_k]

    # === Text Analysis ===

    def get_pos_tags(self, text: str) -> List[Dict[str, str]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å POS —Ç–µ–≥–∏ (—á–∞—Å—Ç–∏ —Ä–µ—á–∏)

        :param text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        :return: –°–ø–∏—Å–æ–∫ {token, pos, tag}
        """
        if not self.is_ready():
            return []

        doc = self.nlp(text)
        return [
            {
                "token": token.text,
                "pos": token.pos_,
                "tag": token.tag_
            }
            for token in doc
        ]

    def extract_keywords(
        self,
        text: str,
        top_n: int = 10
    ) -> List[str]:
        """
        –ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —á–∞—Å—Ç–æ—Ç—É –∏ POS —Ç–µ–≥–∏
        :param text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        :param top_n: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        :return: –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        """
        if not self.is_ready():
            return []

        doc = self.nlp(text)

        # –§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –∏ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ
        keywords = [
            token.lemma_.lower()
            for token in doc
            if token.pos_ in ["NOUN", "PROPN", "ADJ"]
            and not token.is_stop
            and len(token.text) > 2
        ]

        # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —á–∞—Å—Ç–æ—Ç—ã
        from collections import Counter
        freq = Counter(keywords)

        # –í–µ—Ä–Ω—É—Ç—å top_n
        return [word for word, count in freq.most_common(top_n)]

    # === Classification ===

    def classify_block_type(self, text: str, title: str = "") -> str:
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –±–ª–æ–∫–∞

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞
        :param text: –¢–µ–∫—Å—Ç –±–ª–æ–∫–∞
        :param title: –ó–∞–≥–æ–ª–æ–≤–æ–∫ –±–ª–æ–∫–∞
        :return: –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º—ã–π —Ç–∏–ø –±–ª–æ–∫–∞
        """
        combined_text = f"{title} {text}".lower()

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –±–ª–æ–∫–æ–≤
        type_patterns = {
            "paragraph": [
                "¬ß", "absatz", "satz", "nummer",
                "vorschrift", "bestimmung", "regelung"
            ],
            "definition": [
                "definition", "bedeutung", "gilt als", "versteht man",
                "bezeichnet", "umfasst", "ist", "sind"
            ],
            "procedure": [
                "verfahren", "antrag", "pr√ºfung", "entscheidung",
                "feststellung", "genehmigung", "ablauf", "schritt"
            ],
            "requirement": [
                "voraussetzung", "erforderlich", "bedingung", "muss",
                "erf√ºllen", "notwendig", "erfordert"
            ],
            "right": [
                "anspruch", "recht", "berechtigt", "k√∂nnen", "darf",
                "zusteht", "beanspruchen"
            ],
            "obligation": [
                "pflicht", "verpflichtung", "muss", "hat zu", "ist verpflichtet",
                "obliegt", "aufgabe"
            ],
            "sanction": [
                "strafe", "busse", "sanktion", "ordnungswidrigkeit",
                "ahndung", "verstoss"
            ]
        }

        # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
        scores = {}
        for block_type, patterns in type_patterns.items():
            score = sum(1 for pattern in patterns if pattern in combined_text)
            if score > 0:
                scores[block_type] = score

        # –í–µ—Ä–Ω—É—Ç—å —Ç–∏–ø —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        if scores:
            return max(scores.items(), key=lambda x: x[1])[0]

        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        return "paragraph"

    def classify_category(self, text: str, entities: Optional[List[Dict]] = None) -> Optional[str]:
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –±–ª–æ–∫–∞

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ NER –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        :param text: –¢–µ–∫—Å—Ç –±–ª–æ–∫–∞
        :param entities: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —É–∂–µ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
        :return: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –∏–ª–∏ None
        """
        text_lower = text.lower()

        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        category_keywords = {
            "employment": [
                "arbeit", "besch√§ftigung", "arbeitsplatz", "beruf",
                "erwerbst√§tigkeit", "job"
            ],
            "health": [
                "gesundheit", "medizin", "behandlung", "therapie",
                "pflege", "rehabilitation", "krankheit"
            ],
            "education": [
                "bildung", "ausbildung", "schule", "studium",
                "qualifizierung", "weiterbildung", "lernen"
            ],
            "social_security": [
                "sozial", "versicherung", "rente", "leistung",
                "unterst√ºtzung", "hilfe", "f√∂rderung"
            ],
            "participation": [
                "teilhabe", "integration", "inklusion", "zugang",
                "barrierefreiheit", "selbstbestimmung"
            ],
            "administration": [
                "verwaltung", "beh√∂rde", "antrag", "verfahren",
                "zust√§ndigkeit", "genehmigung"
            ]
        }

        # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        scores = {}
        for category, keywords in category_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            if score > 0:
                scores[category] = score

        # –í–µ—Ä–Ω—É—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º —Å—á—ë—Ç–æ–º
        if scores:
            return max(scores.items(), key=lambda x: x[1])[0]

        return None

    def classify_block(self, text: str, title: str = "") -> Dict[str, Any]:
        """
        –ü–æ–ª–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –±–ª–æ–∫–∞

        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—é –±–ª–æ–∫–∞
        :param text: –¢–µ–∫—Å—Ç –±–ª–æ–∫–∞
        :param title: –ó–∞–≥–æ–ª–æ–≤–æ–∫ –±–ª–æ–∫–∞
        :return: –°–ª–æ–≤–∞—Ä—å —Å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π
        """
        # –ò–∑–≤–ª–µ—á—å —Å—É—â–Ω–æ—Å—Ç–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        entities = self.extract_entities(text)

        return {
            "type": self.classify_block_type(text, title),
            "category": self.classify_category(text, entities),
            "entities": entities,
            "confidence": 0.7  # –ë–∞–∑–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è rule-based —Å–∏—Å—Ç–µ–º—ã
        }

    # === Summarization ===

    def summarize_text(
        self,
        text: str,
        max_sentences: int = 3,
        method: str = "frequency"
    ) -> str:
        """
        Extractive summarization (—Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏–µ)

        –í—ã–±–∏—Ä–∞–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞
        :param text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        :param max_sentences: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ —Ä–µ–∑—é–º–µ
        :param method: –ú–µ—Ç–æ–¥ –≤—ã–±–æ—Ä–∞ ('frequency' –∏–ª–∏ 'position')
        :return: –†–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        if not self.is_ready():
            # Fallback: –ø—Ä–æ—Å—Ç–æ –≤–µ—Ä–Ω—É—Ç—å –ø–µ—Ä–≤—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            sentences = text.split('. ')
            return '. '.join(sentences[:max_sentences]) + '.'

        doc = self.nlp(text)
        sentences = list(doc.sents)

        if len(sentences) <= max_sentences:
            return text

        if method == "position":
            # –ü—Ä–æ—Å—Ç–æ–π –º–µ—Ç–æ–¥: –ø–µ—Ä–≤—ã–µ N –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            selected = sentences[:max_sentences]
        else:
            # Frequency-based: –≤—ã–±—Ä–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
            # 1. –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —á–∞—Å—Ç–æ—Ç—É –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤
            word_freq = {}
            for token in doc:
                if (not token.is_stop and not token.is_punct and
                    token.pos_ in ["NOUN", "PROPN", "VERB", "ADJ"]):
                    lemma = token.lemma_.lower()
                    word_freq[lemma] = word_freq.get(lemma, 0) + 1

            # 2. –û—Ü–µ–Ω–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            sentence_scores = []
            for i, sent in enumerate(sentences):
                score = 0
                for token in sent:
                    lemma = token.lemma_.lower()
                    score += word_freq.get(lemma, 0)

                # –ë–æ–Ω—É—Å –¥–ª—è –ø–µ—Ä–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
                if i < 2:
                    score *= 1.5

                sentence_scores.append((score, i, sent))

            # 3. –í—ã–±—Ä–∞—Ç—å top N –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            sentence_scores.sort(reverse=True, key=lambda x: x[0])
            selected_with_idx = sentence_scores[:max_sentences]

            # 4. –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –∏—Å—Ö–æ–¥–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É
            selected_with_idx.sort(key=lambda x: x[1])
            selected = [sent for _, _, sent in selected_with_idx]

        # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        summary = ' '.join(sent.text.strip() for sent in selected)
        return summary

    def generate_summary_points(self, text: str, max_points: int = 5) -> List[str]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ø–∏—Å–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö –ø—É–Ω–∫—Ç–æ–≤

        :param text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        :param max_points: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—É–Ω–∫—Ç–æ–≤
        :return: –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö –ø—É–Ω–∫—Ç–æ–≤
        """
        if not self.is_ready():
            # Fallback: —Ä–∞–∑–±–∏—Ç—å –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            sentences = text.split('. ')
            return sentences[:max_points]

        doc = self.nlp(text)
        sentences = list(doc.sents)

        # –ò–∑–≤–ª–µ—á—å –∫–æ—Ä–æ—Ç–∫–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        points = []
        for sent in sentences:
            sent_text = sent.text.strip()

            # –§–∏–ª—å—Ç—Ä: –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –∏ –Ω–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ
            if 10 < len(sent_text) < 200:
                points.append(sent_text)

            if len(points) >= max_points:
                break

        return points

    def get_text_stats(self, text: str) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–µ–∫—Å—Ç–∞

        :param text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        :return: –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        if not self.is_ready():
            words = text.split()
            return {
                "char_count": len(text),
                "word_count": len(words),
                "sentence_count": text.count('.') + text.count('!') + text.count('?'),
                "avg_word_length": sum(len(w) for w in words) / len(words) if words else 0
            }

        doc = self.nlp(text)

        return {
            "char_count": len(text),
            "word_count": len([t for t in doc if not t.is_punct]),
            "sentence_count": len(list(doc.sents)),
            "token_count": len(doc),
            "avg_word_length": sum(len(t.text) for t in doc) / len(doc) if len(doc) > 0 else 0,
            "unique_words": len(set(t.lemma_.lower() for t in doc if not t.is_stop)),
            "entities_count": len(doc.ents)
        }

    async def shutdown(self):
        """–û—Å–≤–æ–±–æ–¥–∏—Ç—å —Ä–µ—Å—É—Ä—Å—ã"""
        self.nlp = None
        self.embedder = None
        self.initialized = False


# Singleton instance
nlp_service = NLPService()
